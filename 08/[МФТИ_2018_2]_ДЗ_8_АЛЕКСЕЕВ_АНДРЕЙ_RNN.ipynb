{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Упражнение, для реализации \"Ванильной\" RNN\n",
    "* Попробуем обучить сеть восстанавливать слово hello по первой букве. т.е. построим charecter-level модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((3,3))*3\n",
    "b = torch.ones((3,3))*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45., 45., 45.],\n",
       "        [45., 45., 45.],\n",
       "        [45., 45., 45.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 15., 15.],\n",
       "        [15., 15., 15.],\n",
       "        [15., 15., 15.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'ololoasdasddqweqw123456789'\n",
    "# word = 'hello'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет. \n",
    "Позволяет:\n",
    "* Закодировать символ при помощи one-hot\n",
    "* Делать итератор по слову, которыей возвращает текущий символ и следующий как таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataSet:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.chars2idx = {}\n",
    "        self.indexs  = []\n",
    "        for c in word: \n",
    "            if c not in self.chars2idx:\n",
    "                self.chars2idx[c] = len(self.chars2idx)\n",
    "                \n",
    "            self.indexs.append(self.chars2idx[c])\n",
    "            \n",
    "        self.vec_size = len(self.chars2idx)\n",
    "        self.seq_len  = len(word)\n",
    "        \n",
    "    def get_one_hot(self, idx):\n",
    "        x = torch.zeros(self.vec_size)\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return zip(self.indexs[:-1], self.indexs[1:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq_len\n",
    "    \n",
    "    def get_char_by_id(self, id):\n",
    "        for c, i in self.chars2idx.items():\n",
    "            if id == i: return c\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация базовой RNN\n",
    "<br/>\n",
    "Скрытый элемент\n",
    "$$ h_t= tanh⁡ (W_{ℎℎ} h_{t−1}+W_{xh} x_t) $$\n",
    "Выход сети\n",
    "\n",
    "$$ y_t = W_{hy} h_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(VanillaRNN, self).__init__()        \n",
    "        self.x2hidden    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden      = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.activation  = nn.Tanh()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        hidden = self.activation(self.x2hidden(x) + self.hidden(prev_hidden))\n",
    "#         Версия без активации - может происходить gradient exploding\n",
    "#         hidden = self.x2hidden(x) + self.hidden(prev_hidden)\n",
    "        output = self.outweight(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация переменных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = VanillaRNN(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 100\n",
    "optim     = SGD(rnn.parameters(), lr = 0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.41444396972656\n",
      "Clip gradient :  6.128253571447335\n",
      "57.18257522583008\n",
      "Clip gradient :  8.708175942105852\n",
      "49.658294677734375\n",
      "Clip gradient :  10.375445992498053\n",
      "37.342552185058594\n",
      "Clip gradient :  10.933765324504696\n",
      "29.617595672607422\n",
      "Clip gradient :  4.913331218988951\n",
      "23.910120010375977\n",
      "Clip gradient :  3.67594654777344\n",
      "22.12882423400879\n",
      "Clip gradient :  2.7630521407530537\n",
      "22.689374923706055\n",
      "Clip gradient :  23.492668841825783\n",
      "21.24439811706543\n",
      "Clip gradient :  4.46421624044511\n",
      "27.220043182373047\n",
      "Clip gradient :  12.977829952388593\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.hidden.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "#     print(\"Params : \")\n",
    "#     num_params = 0\n",
    "#     for item in rnn.parameters():\n",
    "#         num_params += 1\n",
    "#         print(item.grad)\n",
    "#     print(\"NumParams :\", num_params)\n",
    "#     print(\"Optimize\")\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t oasdasdasdasdasdasdasdasda\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-612b341693d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Prediction:\\t'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mpredword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Original:\\t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredword\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.hidden.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ\n",
    "Реализовать LSTM и GRU модули, обучить их предсказывать тестовое слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тестовое слово\n",
    "word = 'ololoasdasddqweqw123456789'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Написать реализацию LSTM и обучить предсказывать слово\n",
    "\n",
    "class myLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(myLSTM, self).__init__()    \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        self.cand_x2hid = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.cand_hid2hid = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.ix2hid = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.ihid2hid = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.fx2hid = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.fhid2hid = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.ox2hid = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.ohid2hid = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.cand_x2hid = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.cand_hid2hid = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.outweight = nn.Linear(in_features=hidden_size, out_features=out_size, bias=False)\n",
    "    \n",
    "    def forward(self, x, prev_hidden, prev_state):\n",
    "        candidate_cell_state = self.tanh(self.cand_x2hid(x) + self.cand_hid2hid(prev_hidden))\n",
    "        \n",
    "        input_gate = self.sig(self.ix2hid(x) + self.ihid2hid(prev_hidden))\n",
    "        forget_gate = self.sig(self.fx2hid(x) + self.fhid2hid(prev_hidden))\n",
    "        output_gate = self.sig(self.ox2hid(x) + self.ohid2hid(prev_hidden))\n",
    "        \n",
    "        state = forget_gate * prev_state + input_gate * candidate_cell_state\n",
    "        hidden = output_gate * self.tanh(state)\n",
    "        \n",
    "        output = self.outweight(hidden)\n",
    "        \n",
    "        return output, hidden, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "hidden_size=20\n",
    "rnn = myLSTM(in_size=ds.vec_size, hidden_size=hidden_size, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 100\n",
    "optim     = SGD(rnn.parameters(), lr = 0.2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.00183868408203\n",
      "Clip gradient :  1.8934135753720187\n",
      "50.348941802978516\n",
      "Clip gradient :  8.280644278677135\n",
      "24.195974349975586\n",
      "Clip gradient :  18.488357973906677\n",
      "35.21298599243164\n",
      "Clip gradient :  37.739387362514904\n",
      "19.048065185546875\n",
      "Clip gradient :  9.959193190069682\n",
      "7.041546821594238\n",
      "Clip gradient :  6.237555687994236\n",
      "2.2635979652404785\n",
      "Clip gradient :  2.1892817734471004\n",
      "0.25054073333740234\n",
      "Clip gradient :  0.27804219792901924\n",
      "0.05487251281738281\n",
      "Clip gradient :  0.056126192059905725\n",
      "0.024082183837890625\n",
      "Clip gradient :  0.02374052989068462\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(hidden_size)\n",
    "    state = torch.zeros(hidden_size)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh, state = rnn(x, hh, state)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(hidden_size)\n",
    "state = torch.zeros(hidden_size)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh, state = rnn(x, hh, state)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Написать реализацию GRU и обучить предсказывать слово\n",
    "\n",
    "class myGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(myGRU, self).__init__()    \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        self.ux2hid = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.uhid2hid = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.rx2hid = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.rhid2hid = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.hid_cand_x2hid = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hid_cand_hid2hid = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.outweight = nn.Linear(in_features=hidden_size, out_features=out_size, bias=False)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        update_gate = self.sig(self.ux2hid(x) + self.uhid2hid(prev_hidden))\n",
    "        reset_gate = self.sig(self.rx2hid(x) + self.rhid2hid(prev_hidden))\n",
    "        hidden_candidate = self.tanh(self.hid_cand_x2hid(x) + self.hid_cand_hid2hid(reset_gate * prev_hidden))\n",
    "        \n",
    "        hidden = (1 - update_gate) * hidden_candidate + update_gate * prev_hidden\n",
    "        \n",
    "        output = self.outweight(hidden)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "hidden_size=20\n",
    "rnn = myGRU(in_size=ds.vec_size, hidden_size=hidden_size, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 100\n",
    "optim     = SGD(rnn.parameters(), lr = 0.2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.51126098632812\n",
      "Clip gradient :  4.077464295536491\n",
      "36.13667297363281\n",
      "Clip gradient :  11.29450096116869\n",
      "13.686416625976562\n",
      "Clip gradient :  9.713353079763419\n",
      "4.613590240478516\n",
      "Clip gradient :  7.528328063262426\n",
      "3.789278984069824\n",
      "Clip gradient :  7.043000781782226\n",
      "2.5595197677612305\n",
      "Clip gradient :  5.9222902054146935\n",
      "0.18115806579589844\n",
      "Clip gradient :  0.38413490844312564\n",
      "0.04615211486816406\n",
      "Clip gradient :  0.07454732159372253\n",
      "0.018060684204101562\n",
      "Clip gradient :  0.02947075543827954\n",
      "0.011266708374023438\n",
      "Clip gradient :  0.016750137330578\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(hidden_size)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(hidden_size)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
